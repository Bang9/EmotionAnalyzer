{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseReviewCorpus():\n",
    "    from konlpy.tag import Kkma\n",
    "    twitter = Twitter()\n",
    "    kkma = Kkma()\n",
    "\n",
    "    data = open('./review_corpus/ratings_test.txt','r',encoding='UTF-8')\n",
    "\n",
    "    output = open(\"review_data.txt\", 'w',encoding='UTF-8')\n",
    "\n",
    "    header = data.readline().split('\\n')[0].split('\\t')\n",
    "\n",
    "    for review in data:\n",
    "        rv = review.split('\\n')[0].split('\\t')\n",
    "        rv[1] = kkma.pos(rv[1])\n",
    "        t=[]\n",
    "        for item in rv[1]:\n",
    "            t.append(\"/\".join(item))\n",
    "        rv[1] = \" \".join(t)\n",
    "        output.write(\"\\t\".join(rv)+'\\n')\n",
    "\n",
    "#     for review in test_data:\n",
    "#         rv = review.split('\\n')[0].split('\\t')\n",
    "#         rv[1] = kkma.pos(rv[1])\n",
    "#         t=[]\n",
    "#         for item in rv[1]:\n",
    "#             t.append(\"/\".join(item))\n",
    "#         rv[1] = \" \".join(t)\n",
    "#         test.write(\"\\t\".join(rv)+'\\n')\n",
    "\n",
    "    data.close()\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "java.lang.OutOfMemoryErrorPyRaisable",
     "evalue": "java.lang.OutOfMemoryError: Java heap space",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mjava.lang.OutOfMemoryErrorPyRaisable\u001b[0m      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-0d10a2ea54ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mparseReviewCorpus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-38-ce311d5cb315>\u001b[0m in \u001b[0;36mparseReviewCorpus\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mreview\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mrv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreview\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\t'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mrv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkkma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0mt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\bang9\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\konlpy\\tag\\_kkma.py\u001b[0m in \u001b[0;36mpos\u001b[1;34m(self, phrase, flatten)\u001b[0m\n\u001b[0;32m     52\u001b[0m         :param flatten: If False, preserves eojeols.\"\"\"\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m         \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjki\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmorphAnalyzer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphrase\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m         \u001b[0mmorphemes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mjava.lang.OutOfMemoryErrorPyRaisable\u001b[0m: java.lang.OutOfMemoryError: Java heap space"
     ]
    }
   ],
   "source": [
    "parseReviewCorpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33445 33445\n"
     ]
    }
   ],
   "source": [
    "review_data = open(\"review_data.txt\", 'r',encoding='UTF-8')\n",
    "\n",
    "data = []\n",
    "target = []\n",
    "target_names = ['negative','positive']\n",
    "\n",
    "while True:\n",
    "    line = review_data.readline()\n",
    "    if not line: break\n",
    "    if line.split('\\n')[0].split('\\t')[2] == '0':\n",
    "        target.append(0)\n",
    "    else:\n",
    "        target.append(1)\n",
    "    data.append(line.split('\\t')[1])\n",
    "    \n",
    "print(len(data),len(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "#load polarity dictionary\n",
    "file = open('./dic/polarity.csv','r',encoding='UTF-8')\n",
    "reader = csv.reader(file)\n",
    "\n",
    "#get header\n",
    "header = reader.__next__()\n",
    "\n",
    "#make dictionary\n",
    "neg_dic = [[],[],[],[],[],[],[],[],[]]  #부정\n",
    "pos_dic = [[],[],[],[],[],[],[],[],[]]  #긍정\n",
    "\n",
    "for line in reader:\n",
    "    line[0] = line[0].split(';') #split ngram\n",
    "    maxValue = line[7]\n",
    "    \n",
    "    if maxValue == 'POS':\n",
    "        for i in range(len(header)):\n",
    "            pos_dic[i].append(line[i])\n",
    "    elif maxValue == 'NEG':\n",
    "        for i in range(len(header)):\n",
    "            neg_dic[i].append(line[i])\n",
    "            \n",
    "\n",
    "\n",
    "def findPOS(unigram,bigram,trigram):\n",
    "    pos_val = 0\n",
    "    count=0\n",
    "    for uni in unigram:\n",
    "        if uni in pos_dic[0]:\n",
    "            mvPos=pos_dic[0].index(uni) #max value position\n",
    "            mvProp=pos_dic[8][mvPos] #max value prop\n",
    "            pos_val += float(mvProp)\n",
    "            count+=1\n",
    "    for bi in bigram:\n",
    "        if bi in pos_dic[0]:\n",
    "            mvPos=pos_dic[0].index(bi) #max value position\n",
    "            mvProp=pos_dic[8][mvPos] #max value prop\n",
    "            pos_val += float(mvProp)\n",
    "            count+=1\n",
    "    for tri in trigram:\n",
    "        if tri in pos_dic[0]:\n",
    "            mvPos=pos_dic[0].index(tri) #max value position\n",
    "            mvProp=pos_dic[8][mvPos] #max value prop\n",
    "            pos_val += float(mvProp)\n",
    "            count+=1\n",
    "            \n",
    "    return [pos_val,count]\n",
    "\n",
    "def findNEG(unigram,bigram,trigram):\n",
    "    neg_val = 0\n",
    "    count=0\n",
    "    for uni in unigram:\n",
    "        if uni in neg_dic[0]:\n",
    "            mvPos=neg_dic[0].index(uni) #max value position\n",
    "            mvProp=neg_dic[8][mvPos] #max value prop\n",
    "            neg_val += float(mvProp)\n",
    "            count+=1\n",
    "    for bi in bigram:\n",
    "        if bi in neg_dic[0]:\n",
    "            mvPos=neg_dic[0].index(bi) #max value position\n",
    "            mvProp=neg_dic[8][mvPos] #max value prop\n",
    "            neg_val += float(mvProp)\n",
    "            count+=1\n",
    "    for tri in trigram:\n",
    "        if tri in neg_dic[0]:\n",
    "            mvPos=neg_dic[0].index(tri) #max value position\n",
    "            mvProp=neg_dic[8][mvPos] #max value prop\n",
    "            neg_val += float(mvProp)\n",
    "            count+=1\n",
    "            \n",
    "    return [neg_val,count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(sentence):\n",
    "    wordsSet = {} #모든 단어의 카운팅 object\n",
    "    \n",
    "    import re\n",
    "    import operator\n",
    "    import numpy as np\n",
    "    \n",
    "    #모든 문자열 갯수 카운팅\n",
    "    for term in sentence:\n",
    "        for word in term.split(' '):\n",
    "            if word in wordsSet :\n",
    "                wordsSet[word]+=1\n",
    "            else :\n",
    "                wordsSet[word]=1\n",
    "    \n",
    "    #get all_words, 모든 단어의 배열\n",
    "    #많이 사용된 상위 25개 제거\n",
    "    all_words = []\n",
    "    stop_words = []\n",
    "    \n",
    "    sortedWords = sorted(wordsSet.items(), key=operator.itemgetter(1))\n",
    "    for i in range(len(sortedWords), len(sortedWords)):\n",
    "        stop_words.append(sortedWords[i][0])     \n",
    "    for i in range(0, len(sortedWords)):\n",
    "        all_words.append(sortedWords[i][0])        \n",
    "    all_words.sort()\n",
    "    \n",
    "    #wordsSet index\n",
    "    wordsSet_index = {}\n",
    "    i=0\n",
    "    for word in all_words:\n",
    "        wordsSet_index[word]=i\n",
    "        i+=1\n",
    "\n",
    "    #하나씩 돌면서 corpus_feature생성\n",
    "    corpus_feature = np.zeros((len(sentence),len(all_words)+2))\n",
    "    j=0\n",
    "    for term in sentence:\n",
    "        unigram = term.split(' ')\n",
    "        bigram = []\n",
    "        trigram = []\n",
    "        for i in range(len(unigram)):\n",
    "            if i==len(unigram)-1:\n",
    "                break\n",
    "            p = []\n",
    "            p.append(unigram[i])\n",
    "            p.append(unigram[i+1])\n",
    "            bigram.append(p)\n",
    "        for i in range(2,len(unigram)):\n",
    "            p = []\n",
    "            p.append(unigram[i-2])\n",
    "            p.append(unigram[i-1])\n",
    "            p.append(unigram[i])\n",
    "            trigram.append(p)\n",
    "        \n",
    "        for word in unigram:\n",
    "            if(word not in stop_words and word != \"\"):\n",
    "                corpus_feature[j][wordsSet_index[word]]+=1\n",
    "        \n",
    "        #if negative append 0\n",
    "        #elif positive append 1\n",
    "        #print(findPOS(unigram,bigram,trigram))\n",
    "        if(findPOS(unigram,bigram,trigram)[1] < findNEG(unigram,bigram,trigram)[1]):\n",
    "            corpus_feature[j][len(corpus_feature[0])-1]=0\n",
    "            corpus_feature[j][len(corpus_feature[0])-2]=1\n",
    "        elif(findPOS(unigram,bigram,trigram)[1] > findNEG(unigram,bigram,trigram)[1]):\n",
    "            corpus_feature[j][len(corpus_feature[0])-1]=1\n",
    "            corpus_feature[j][len(corpus_feature[0])-2]=0\n",
    "        j+=1\n",
    "    \n",
    "    return corpus_feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "(2000, 5484)\n",
      "1500 500\n"
     ]
    }
   ],
   "source": [
    "corpus_feature = vectorize(data[:2000])\n",
    "print(len(corpus_feature))\n",
    "#split test case\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(corpus_feature, target[:1000], test_size=0.2)\n",
    "\n",
    "X_train=corpus_feature[:1500]\n",
    "X_test=corpus_feature[1500:2000]\n",
    "y_train=target[:1500]\n",
    "y_test=target[1500:2000]\n",
    "\n",
    "print(np.shape(corpus_feature))\n",
    "print (len(X_train), len(X_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "   negative       0.73      0.74      0.73       244\n",
      "   positive       0.75      0.73      0.74       256\n",
      "\n",
      "avg / total       0.74      0.74      0.74       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Learning\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC(kernel = 'linear')\n",
    "clf.fit(X_train, y_train)\n",
    "#X_train과 y_train으로 SVC-linear로 학습\n",
    "\n",
    "# Evaluation\n",
    "y_pred = clf.predict(X_test)\n",
    "#test셋으로 학습된 clf를 이용해서 예측데이터를 생성\n",
    "\n",
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
